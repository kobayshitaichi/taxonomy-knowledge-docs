In recent years, large-scale language models such as ChatGPT have attracted much attention. However, while these models have high language processing performance due to the vast amount of knowledge stored in them, it is said that the energy required for learning is equivalent to the power of one nuclear power plant for one hour. In addition, large-scale GPU clusters are required for operation, and the costs of tuning and inference to specialize for various industries are enormous, so there are issues with sustainability and the economic burden for companies to prepare a learning environment. NTT has been conducting research and development to solve these issues, and has now developed "tsuzumi", a large-scale language model that is lightweight yet has world-class Japanese processing performance. "tsuzumi" has a parameter size of 60 to 7 billion, reducing the costs required for learning and tuning, which are issues for cloud-provided LLMs in the city. "tsuzumi" supports English and Japanese, and achieves inference operation on one GPU or CPU. In addition, "tsuzumi" supports modalities such as vision and hearing, and can be tuned to be specialized for specific industries and corporate organizations. The NTT Group plans to launch commercial services using "tsuzumi" in March 2024, and is promoting future research and development of "tsuzumi" to create new value by adding further multimodal functions . 

Since the appearance of ChatGPT, released by OpenAI on November 30, 2022, interactive AI that supports natural interaction with users and can handle a wider range of tasks than humans on many topics has attracted much attention. The language models that enable such interactive AI perform well on natural language generation tasks by using models with a large number of parameters that have been trained using large data sets and computational resources. While this increase in parameters leads to improved performance, it also comes with challenges such as increased computational resources and energy consumption. For example, it is said that training a GPT-3 scale model requires approximately 1300 MWh of electricity (equivalent to one hour of power for one nuclear power plant). Instead of increasing the parameter size, “tsuzumi” takes the approach of improving the quality and quantity of the Japanese training data to achieve high Japanese processing power with a very lightweight model.
As of March 2024, tsuzumi will be available in two parameter sizes: 7 billion (7B) for the lightweight version and 600 million (0.6B) for the ultra-lightweight version, approximately 1/300th and 1/25th the size of OpenAI's GPT-3's 175 billion (175B). The lightweight version is available on a single GPU, and the ultra-lightweight version is available on a CPU to allow for fast inference operations, thus reducing the cost of additional training and inference required in practical applications.

NTT Laboratories has accumulated more than 40 years of natural language processing research, and its research capabilities in the field of AI are among the best in the world, ranking 12th in the world and 1st in Japan in terms of the number of papers in the field of AI (by 2022) . In addition, we are the No. 1 company in Japan in the number of papers accepted for publication at top conferences in the field of natural language processing, and the No. 1 company in the number of excellence awards received from the Association for Natural Language Processing over the past 10 years, and we have continued to make achievements over the years.
In particular, by leveraging NTT Laboratories' long-accumulated language processing research, tsuzumi has demonstrated high accuracy in various benchmark comparisons, even with small parameter sizes. The Rakuda benchmark for generative AI outperforms GPT-3.5 by 81.3%, The performance has been steadily improving from the 52.2% win rate as of October 2023.

LLM tuning is the process of tailoring a model's behavior to a specific task or purpose, with the main goal being that the model will produce more accurate and useful responses for a particular task. In general, it is expected to produce more desirable responses by improving task conformance, adding constraints according to user or legal requirements, acquiring domain- or industry-specific knowledge or vocabulary, and stylistic adjustments such as response style and tone.
Re-learning a large number of parameters can be computationally expensive when trying to train an LLM to learn additional new knowledge. After April 2024, multiple adapters can be flexibly switched or combined to create a synergistic effect depending on users and scenarios, From April 2024, we plan to introduce a “multi-adapter” feature that will allow users to flexibly switch between or combine multiple adapters to create a synergistic effect depending on the user and the situation. With this feature, by connecting multiple adapters to a single “tsuzumi” mainframe, a single computing process can perform processing according to multiple tuning targets, thereby reducing the cost of providing the service. This enables, for example, the provision of fine-tuned tuning for specific organizations, positions, and authorities within a company at a low cost.

tsuzumi” supports visual reading to understand graphical representations that are not necessarily verbalized.
In the future, “tsuzumi” will support modal augmentation, in which the robot understands the nuances of speech, facial expressions, the user's situation, and even its own physical senses and the physical characteristics of humans, and can cooperate with people in the real world. Modal augmentation of language + vision will enable the robot to answer not only verbal questions, but also questions while presenting document images. For example, the language model can be used for tasks that require human cognition, such as searching and screening documents with images, such as invoices and manuals, or using AI to compare and evaluate product descriptions and price plan descriptions on websites, which are needed in many aspects of business. It is also expected to contribute to the development of industrially important services such as web search and chatbots.
Several sight-reading benchmarks have achieved higher accuracy than GPT-3.5 and GPT-4 (text input) on more complex evaluation data sets.
NTT has been working on visual reading comprehension since 2020, and has had several papers accepted for the most difficult international conferences and achieved top rankings in international competitions.
